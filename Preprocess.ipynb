{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "euO-jiN2WFcn"
   },
   "source": [
    "#### **Преобразование txt файлов в csv** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C9QxpRFwTIUe"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "import csv\n",
    "\n",
    "# преобразуем наши текстовые файлы в csv файл\n",
    "PATH = \"C:\\\\Users\\\\Asus\\\\Desktop\\\\nan\"  # путь к папке с файлами\n",
    "file_names = os.listdir(PATH) \n",
    "\n",
    "    csv_writer = csv.writer(csv_file, delimiter=',')\n",
    "    n_docs = 0\n",
    "    for name in file_names:\n",
    "        if name.startswith('en'):\n",
    "            n_docs += 1\n",
    "            with codecs.open(PATH + \"/\" + name, encoding = 'utf-8') as f:\n",
    "                result = f.read()\n",
    "                num = ''.join([str(s) for s in list(name) if s.isdigit()])\n",
    "                name_ru = f'ru_paint_{num}.txt'\n",
    "                f_ru = open(f\"C:\\\\Users\\\\Asus\\\\Desktop\\\\nan\\\\{name_ru}\", encoding='utf-8')\n",
    "                result_ru = f_ru.read()\n",
    "                csv_writer.writerow([result]+[result_ru])\n",
    "print(n_docs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e3_S0d5TTwq6"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# добавляем названия к колонкам\n",
    "df = pd.read_csv(\"paint0204.csv\", header=None)\n",
    "df.to_csv(\"paint0204.csv\", header=[\"en\", \"ru\"], index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3nSS-tvgWsag"
   },
   "source": [
    "### Предобработка текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z5sYvUPQXLei"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# длина документов до обработки\n",
    "corpus = pd.read_csv(\"paint0204.csv\")\n",
    "corpus['len_en'] = corpus['en'].apply(lambda x: len(str(x).split()))\n",
    "corpus['len_ru'] = corpus['ru'].apply(lambda x: len(str(x).split()))\n",
    "corpus.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DhGT6Trziq5C"
   },
   "outputs": [],
   "source": [
    "import re \n",
    "import nltk\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "STW_PATH = '/Users/ASUS/Desktop/swl_optimum.txt' \n",
    "file = open(STW_PATH, encoding = 'utf-8')\n",
    "stw_ru = file.read().split()[1:]\n",
    "stw_en = set(stopwords.words('english'))\n",
    "\n",
    "def english(text):\n",
    "    \"\"\"\n",
    "    Преобработка текстов на английском языке. \n",
    "    Приводим к нижнему регистру, \n",
    "    удаляем цифры и знаки препинания,\n",
    "    удаляем стоп-слова, \n",
    "    нормализация слов (лемматизация с spacy), \n",
    "    выделяем биграммы?\n",
    "    \"\"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^\\w\\s]|\\d','', text)\n",
    "    text = word_tokenize(text)\n",
    "    \n",
    "    text = [word for word in text if not word in stw_en]\n",
    "    text = ' '.join(text)\n",
    "    hel = nlp(text)\n",
    "    text = [token.lemma_ for token in hel]\n",
    "\n",
    "    # text = [ps.stem(w) for w in text]\n",
    "    \n",
    "    print(\"**\")\n",
    "    return ' '.join(text)\n",
    "\n",
    "\n",
    "def russian(text):\n",
    "    \"\"\"\n",
    "    Преобработка текстов на русском языке. \n",
    "    Приводим к нижнему регистру, \n",
    "    удаляем цифры и знаки препинания,\n",
    "    удаляем стоп-слова, \n",
    "    нормализация слов (скорее всего лемматизация через pymorphy2), \n",
    "    выделяем биграммы?\n",
    "    \"\"\"\n",
    "    res = ''\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^\\w\\s]|\\d','', text)\n",
    "    text = word_tokenize(text)\n",
    "    \n",
    "    for word in text:                         \n",
    "        p = morph.parse(word)[0]\n",
    "        res = res + ' ' + p.normal_form # лемматизация\n",
    "    text = [word for word in res.split() if not word in stw_ru]\n",
    "    \n",
    "    print(\"**\")\n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ari4CnRTjDZD"
   },
   "outputs": [],
   "source": [
    "corpus['en'] = corpus['en'].apply(english)\n",
    "corpus['ru'] = corpus['ru'].apply(russian)\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cp90yhrzjHQ5"
   },
   "outputs": [],
   "source": [
    "# длина документов после обрабоки\n",
    "corpus['len_en_stem'] = corpus['en'].apply(lambda x: len(str(x).split()))\n",
    "corpus['len_ru_lem'] = corpus['ru'].apply(lambda x: len(str(x).split()))\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eCiAFFCQ87wc"
   },
   "outputs": [],
   "source": [
    "# corpus.to_csv(\"lemstem_2try.csv\", header=[\"en\", \"ru\", \"len_en\", \"len_ru\", \"len_en_stem\", \"len_ru_lem\"], index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mRnhUv1Wjtrl"
   },
   "source": [
    "## Выделение биграмм и триграмм\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1dV5haa8Ir5"
   },
   "source": [
    "### Для русского языка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VNOuMfs-8ZB3"
   },
   "outputs": [],
   "source": [
    "text_ru = []\n",
    "for index, row in corpus.iterrows():\n",
    "        text_ru.append(row['ru'].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R1-sOpEW8Nte"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "bigram = Phrases(text_ru, scoring='npmi', threshold=0.4, min_count=5) \n",
    "\n",
    "for idx in range(len(text_ru)):\n",
    "    for token in bigram[text_ru[idx]]:\n",
    "        if '_' in token:\n",
    "            # print(token)\n",
    "            text_ru[idx].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W-zX6bDs8jzk"
   },
   "outputs": [],
   "source": [
    "texts = [' '.join(text) for text in text_ru]\n",
    "corpus['ru'] = texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hN0PbpNt8nL2"
   },
   "source": [
    "### Для английского языка\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z6HOrF148s3g"
   },
   "outputs": [],
   "source": [
    "text_en = []\n",
    "for index, row in corpus.iterrows():\n",
    "        text_en.append(row['en'].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nbnsie0e-Xvx"
   },
   "outputs": [],
   "source": [
    "bigram = Phrases(text_en, scoring='npmi', threshold=0.4, min_count=5) \n",
    "\n",
    "for idx in range(len(text_en)):\n",
    "    for token in bigram[text_en[idx]]:\n",
    "        if '_' in token:\n",
    "            # print(token)\n",
    "            text_en[idx].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6tVpyJwF-mq-"
   },
   "outputs": [],
   "source": [
    "texts_en = [' '.join(text) for text in text_en]\n",
    "corpus['en'] = texts_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9xSMjHtg-wC0"
   },
   "outputs": [],
   "source": [
    "corpus['len_en_bi'] = corpus['en'].apply(lambda x: len(str(x).split()))\n",
    "corpus['len_ru_bi'] = corpus['ru'].apply(lambda x: len(str(x).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G1k2UlVX-2wp"
   },
   "outputs": [],
   "source": [
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CCiR4OZW-5vt"
   },
   "outputs": [],
   "source": [
    "corpus.to_csv(\"wiki_prep.csv\", header=[\"en\", \"ru\", \"len_en\", \"len_ru\", \"len_en_stem\", \"len_ru_lem\", \"len_en_bi\", \"len_ru_bi\"], index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOOY8sQqF356vrQAyNpi3Yv",
   "name": "Preprocess.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
